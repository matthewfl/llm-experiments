{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3e033-5b18-4c94-8e95-6baf285ae8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bde2a0e-8a60-41b0-929b-3aba6aa447da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cfd649ab21488fa58fd1e071c6f531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/41.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6806da9cebf4d4daf7da6f03a014aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff919fc3e7c40bf9294bdc31e3c5106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f098fe242669417eb2112152b90497c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e29e7309def4f10b10f8de59e197826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c452954ba94cd8910ea208ece0b87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433eb3b6445d4d918d4cbeeac9215a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n",
      "/home/matthew/.virtualenvs/llm-experiments/lib/python3.11/site-packages/transformers/generation/utils.py:1181: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_hello_world():\n",
      "    print(\"Hello World!\")\n",
      "\n",
      "def print_hello_\n"
     ]
    }
   ],
   "source": [
    "# copied from https://huggingface.co/bigcode/starcoder2-7b\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "checkpoint = \"bigcode/starcoder2-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# for fp16 use `torch_dtype=torch.float16` instead\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, low_cpu_mem_usage=False)\n",
    "\n",
    "inputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\")#.to(\"cuda\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f06649f-deff-463f-8af2-cb216c93037a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c96e6746-73c4-4749-b151-e113ad078a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfca75d9-f710-4c60-b4c1-cbc965f79a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Starcoder2ForCausalLM(\n",
       "  (model): Starcoder2Model(\n",
       "    (embed_tokens): Embedding(49152, 4608)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Starcoder2DecoderLayer(\n",
       "        (self_attn): Starcoder2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=4608, out_features=4608, bias=True)\n",
       "          (k_proj): Linear(in_features=4608, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=4608, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=4608, out_features=4608, bias=True)\n",
       "          (rotary_emb): Starcoder2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Starcoder2MLP(\n",
       "          (c_fc): Linear(in_features=4608, out_features=18432, bias=True)\n",
       "          (c_proj): Linear(in_features=18432, out_features=4608, bias=True)\n",
       "          (act): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4608, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1cba80b-0199-46f3-9c21-4a3a9378b21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "gg = model.generate(outputs, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "650e9dfb-fff3-4fc5-8908-cc8cdb3971c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  610,  1489,   100,  7670,   100,  5879,  2284,   303,  1489,   459,\n",
       "          8302, 10914, 16013,   222,   222,   610,  1489,   100,  7670,   100,\n",
       "          5879,   100,  1814,   100,   444,    45,   444,   731,   303,  1489,\n",
       "           459,  8302,   332,   494,   655,   494,   332, 16013,   222,   222,\n",
       "           610,  1489,   100,  7670,   100,  5879,   100,  1814,   100,   444,\n",
       "           100,   382,   100,   400,    45,   444,    49, 11505,   731,   303,\n",
       "          1489,   459,  8302,   332,   494,   655,   494,   332, 16013,   303,\n",
       "          1489,   459,  4296,   904,   332,   494,   615,    45,   400,    46,\n",
       "           494,   332, 11339,  3627,  7164,   222,   222,   610,  1489,   100,\n",
       "          7670,   100,  5879,   100,  1814,   100,   444,   100,   382,   100]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48fa5893-b3f5-46a6-b3b7-770b9f04c468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_hello_world():\n",
      "    print(\"Hello World!\")\n",
      "\n",
      "def print_hello_world_with_name(name):\n",
      "    print(\"Hello \" + name + \"!\")\n",
      "\n",
      "def print_hello_world_with_name_and_age(name, age):\n",
      "    print(\"Hello \" + name + \"!\")\n",
      "    print(\"You are \" + str(age) + \" years old.\")\n",
      "\n",
      "def print_hello_world_with_name_and_\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(gg[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8595b56-d14f-41b0-82a2-10e78ba6d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = '''\n",
    "def broken():\n",
    "    print(a)\n",
    "    a = 123\n",
    "\n",
    "def fixed():\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0967315c-0f5c-41e9-b286-ca8d52c2c3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "b2 = model.generate(tokenizer.encode(bad, return_tensors='pt'), max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d57cf166-1552-48f3-b767-450f1e4fa178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def broken():\n",
      "    print(a)\n",
      "    a = 123\n",
      "\n",
      "def fixed():\n",
      "\tglobal a\n",
      "\ta = 123\n",
      "\n",
      "broken()\n",
      "fixed()\n",
      "<file_sep>/python/python_basics/0\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(b2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cf7af1d-8e27-4dd0-ae3a-884bdbd48ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "b3 = model.generate(tokenizer.encode(bad, return_tensors='pt'), max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "001afded-906b-445c-a4d3-4fce20ff773c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method generate in module transformers.generation.utils:\n",
      "\n",
      "generate(inputs: Optional[torch.Tensor] = None, generation_config: Optional[transformers.generation.configuration_utils.GenerationConfig] = None, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None, synced_gpus: Optional[bool] = None, assistant_model: Optional[ForwardRef('PreTrainedModel')] = None, streamer: Optional[ForwardRef('BaseStreamer')] = None, negative_prompt_ids: Optional[torch.Tensor] = None, negative_prompt_attention_mask: Optional[torch.Tensor] = None, **kwargs) -> Union[transformers.generation.utils.GenerateDecoderOnlyOutput, transformers.generation.utils.GenerateEncoderDecoderOutput, transformers.generation.utils.GenerateBeamDecoderOnlyOutput, transformers.generation.utils.GenerateBeamEncoderDecoderOutput, torch.LongTensor] method of transformers.models.starcoder2.modeling_starcoder2.Starcoder2ForCausalLM instance\n",
      "    Generates sequences of token ids for models with a language modeling head.\n",
      "    \n",
      "    <Tip warning={true}>\n",
      "    \n",
      "    Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n",
      "    model's default generation configuration. You can override any `generation_config` by passing the corresponding\n",
      "    parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n",
      "    \n",
      "    For an overview of generation strategies and code examples, check out the [following\n",
      "    guide](../generation_strategies).\n",
      "    \n",
      "    </Tip>\n",
      "    \n",
      "    Parameters:\n",
      "        inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
      "            The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
      "            method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
      "            should be in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
      "            `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
      "        generation_config (`~generation.GenerationConfig`, *optional*):\n",
      "            The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n",
      "            passed to generate matching the attributes of `generation_config` will override them. If\n",
      "            `generation_config` is not provided, the default will be used, which has the following loading\n",
      "            priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n",
      "            configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n",
      "            default values, whose documentation should be checked to parameterize generation.\n",
      "        logits_processor (`LogitsProcessorList`, *optional*):\n",
      "            Custom logits processors that complement the default logits processors built from arguments and\n",
      "            generation config. If a logit processor is passed that is already created with the arguments or a\n",
      "            generation config an error is thrown. This feature is intended for advanced users.\n",
      "        stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      "            Custom stopping criteria that complements the default stopping criteria built from arguments and a\n",
      "            generation config. If a stopping criteria is passed that is already created with the arguments or a\n",
      "            generation config an error is thrown. If your stopping criteria depends on the `scores` input, make\n",
      "            sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is\n",
      "            intended for advanced users.\n",
      "        prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
      "            If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "            provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
      "            `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
      "            on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
      "            for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
      "            Retrieval](https://arxiv.org/abs/2010.00904).\n",
      "        synced_gpus (`bool`, *optional*):\n",
      "            Whether to continue running the while loop until max_length. Unless overridden this flag will be set to\n",
      "            `True` under DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished\n",
      "            generating before other GPUs. Otherwise it'll be set to `False`.\n",
      "        assistant_model (`PreTrainedModel`, *optional*):\n",
      "            An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
      "            same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model\n",
      "            is much faster than running generation with the model you're calling generate from. As such, the\n",
      "            assistant model should be much smaller.\n",
      "        streamer (`BaseStreamer`, *optional*):\n",
      "            Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
      "            through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
      "        negative_prompt_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            The negative prompt needed for some processors such as CFG. The batch size must match the input batch\n",
      "            size. This is an experimental feature, subject to breaking API changes in future versions.\n",
      "        negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Attention_mask for `negative_prompt_ids`.\n",
      "        kwargs (`Dict[str, Any]`, *optional*):\n",
      "            Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be\n",
      "            forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n",
      "            specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n",
      "    \n",
      "    Return:\n",
      "        [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
      "        or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
      "    \n",
      "            If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      "            [`~utils.ModelOutput`] types are:\n",
      "    \n",
      "                - [`~generation.GenerateDecoderOnlyOutput`],\n",
      "                - [`~generation.GenerateBeamDecoderOnlyOutput`]\n",
      "    \n",
      "            If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      "            [`~utils.ModelOutput`] types are:\n",
      "    \n",
      "                - [`~generation.GenerateEncoderDecoderOutput`],\n",
      "                - [`~generation.GenerateBeamEncoderDecoderOutput`]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f29cf1a-f7d8-4950-9285-f1224dec0df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def broken():\n",
      "    print(a)\n",
      "    a = 123\n",
      "\n",
      "def fixed():\n",
      "\tglobal a\n",
      "\ta = 123\n",
      "\n",
      "broken()\n",
      "fixed()\n",
      "<file_sep>/python/python_basics/0\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(b3[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99232a81-c059-4028-935f-70fc330fe751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:49152 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "fib = '''\n",
    "(defn fibonacci [n]\n",
    "'''\n",
    "f1 = model.generate(tokenizer.encode(fib, return_tensors='pt'), max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc19a61a-2c44-49fc-9055-5438ca61132d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(defn fibonacci [n]\n",
      "(if (<= n 1)\n",
      "n\n",
      "(+ (fibonacci (- n 1)) (fibonacci (- n 2)))))\n",
      "\n",
      "(fibonacci 10)\n",
      "55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(f1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322e2aec-2a01-45b0-a929-7fd23dc62aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = model.generate("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
